{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fouille de texte avec NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... depuis le web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = urlopen(url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décommenter la ligne en dessous pour l'affichage\n",
    "#print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On nettoie les caractères HTML grâce à BeautifulSoup qui reprend une implémentation précédente de NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html)\n",
    "text2 = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décommenter la ligne en dessous pour l'affichage\n",
    "#print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais il reste beaucoup de caractère spéciaux tels que les retours charriot \\n ou \\r ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = text2.replace('\\n', ' ').replace('\\r', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décommenter la ligne en dessous pour l'affichage\n",
    "#print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ... depuis un fichier stocké sur disque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas d'un fichier .docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise la lib python-docx qui n'est pas présent par défaut sur Anaconda... Ii faut donc l'installer via pip. On se met en mode magic cell pour exécuter des commandes linux depuis un Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /home/bestprojectever/anaconda3/lib/python3.7/site-packages (0.8.10)\n",
      "Requirement already satisfied: lxml>=2.3.2 in /home/bestprojectever/anaconda3/lib/python3.7/site-packages (from python-docx) (4.3.2)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(\"Virtual Sprint Big Data paper.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for para in doc.paragraphs:\n",
    "    texts.append(para.text)\n",
    "docxtext = '\\n'.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docxtext = docxtext.replace('\\n', ' ').replace('\\r', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décommenter la ligne en dessous pour l'affichage\n",
    "#print(docxtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction nltk.word_tokenize() permet de tokeniser un texte en se basant sur les espaces et les signes de ponctuation. Tokenisons le texte extrait du .docx précédent ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_word_tokens = word_tokenize(docxtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décommenter la ligne en dessous pour l'affichage\n",
    "#print(docx_word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objet TweetTokenizer est spécialement fait pour tokeniser des tweets (reconnaissance des hashtags et des smileys...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokens = tweet_tokenizer.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décommenter la ligne en dessous pour l'affichage\n",
    "#print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objet MWETokenizer permet de \"retokeniser\" ou \"redistribuer des tokens\" en regroupant les expressions composées comme par exemple \"a little bit\", \"in spite of\" ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer = MWETokenizer([('a', 'little'), ('a', 'little', 'bit'), ('a', 'lot')])\n",
    "mwe_tokenizer.add_mwe(('in', 'spite', 'of'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_test = \"In a little or a little bit or a lot in spite of\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà ce que ça donne avec un tokenizer classique..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(mwe_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'a', 'little', 'or', 'a', 'little', 'bit', 'or', 'a', 'lot', 'in', 'spite', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà ce que ça donne avec un MWE tokenizer ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'a_little', 'or', 'a_little_bit', 'or', 'a_lot', 'in_spite_of']\n"
     ]
    }
   ],
   "source": [
    "print(mwe_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objet RegexpTokenizer permet de tokeniser en se basant sur des expressions régulières"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... soit les tokens correspondent à une expression régulière"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "regex_tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... soit le(s) séparateur(s) correspondent à une expression régulière"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3',\n",
       " '88',\n",
       " 'in New York',\n",
       " '  Please buy me',\n",
       " 'two of them',\n",
       " 'Thanks']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_tokenizer = RegexpTokenizer('\\.|\\n', gaps=True)\n",
    "regex_tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objet Blankline Tokenizer permet de tokeniser ligne par ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BlanklineTokenizer().tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opérations sur les Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.g. filtrage de stopwords et signes de ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous, on calcule la fréquence de chaque token (mot) dans le texte docxtext, elle sont affichées dans l'ordre décroissant du plus fréquent au moins fréquent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({' ': 7921, 'e': 4518, 't': 4316, 'a': 3819, 'i': 3542, 's': 3150, 'o': 2960, 'n': 2697, 'r': 2327, 'l': 1548, ...})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(docxtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous, on filtre les tokens pour ne garder que les mots importants et supprimer les Stopwords (articles, prépositions etc) ainsi que les signes de ponctuation. <br/>\n",
    "Attention à télécharger préalablement les corpus <b> stopwords </b> et <b> punkt </b> depuis <b> nltk.download() </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_docxtext_tokens = [item for item in docx_word_tokens\n",
    "                         if item not in stopwords.words('english') and item not in  string.punctuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalculons les fréquences des tokens restants, et remarquons qu'on obtient en haut de la chaîne des mots qui sont beaucoup plus pertinents et caractéristique du sujet du document (Big Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Data': 166, 'Big': 152, 'data': 124, 'statistical': 123, 'organizations': 93, 'The': 64, 'sources': 47, 'use': 41, 'may': 40, 'information': 34, ...})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(clean_docxtext_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le Part-Of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration des Tagged Corpora de NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Tagging utilisant la fonction pos_tag(). Celle-ci fait appel à l'algorithme du Perceptron pour la reconnaissance des POS Tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(\"And now for something completely different\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'now', 'for', 'something', 'completely', 'different']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(\"They refuse to permit us to obtain the refuse permit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'argument <em> tagset='universal' </em> permet de renvoyer des tags dans un format plus lisible (plus Human Friendly) <br>\n",
    "Ne pas oublier de télécharger le corpus qui contient ces tags à savoir <b> universal_tagset </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRON'),\n",
       " ('refuse', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('permit', 'VERB'),\n",
       " ('us', 'PRON'),\n",
       " ('to', 'PRT'),\n",
       " ('obtain', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('refuse', 'NOUN'),\n",
       " ('permit', 'NOUN')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tokens,tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorons maintenant qq Tagged Corpora de NLTK ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown, nps_chat, treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('Fulton', 'NOUN'),\n",
       " ('County', 'NOUN'),\n",
       " ('Grand', 'ADJ'),\n",
       " ('Jury', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('Friday', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'ADP')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words(tagset='universal')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('now', 'ADV'),\n",
       " ('im', 'PRON'),\n",
       " ('left', 'VERB'),\n",
       " ('with', 'ADP'),\n",
       " ('this', 'DET'),\n",
       " ('gay', 'ADJ'),\n",
       " ('name', 'NOUN'),\n",
       " (':P', 'X'),\n",
       " ('PART', 'VERB'),\n",
       " ('hey', 'X')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nps_chat.tagged_words(tagset='universal')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NOUN'),\n",
       " ('Vinken', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('61', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('old', 'ADJ'),\n",
       " (',', '.'),\n",
       " ('will', 'VERB'),\n",
       " ('join', 'VERB'),\n",
       " ('the', 'DET')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank.tagged_words(tagset='universal')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quel est le tag le plus fréquent dans ces corpora (e.g. brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_fd = FreqDist(tag for (word, tag) in brown_news_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NOUN': 30654, 'VERB': 14399, 'ADP': 12355, '.': 11928, 'DET': 11389, 'ADJ': 6706, 'ADV': 3349, 'CONJ': 2717, 'PRON': 2535, 'PRT': 2264, ...})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que NOUN est le genre / tag le plus fréquent dont on peut se servir comme Tag par défaut !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples de Taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Tagger permet de tout tagger de la même manière. Si on veut le moins se tromper, choisir <b> NOM </b> comme tag par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import DefaultTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = 'I do not like green eggs and ham, I do not like them Sam I am!'\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'NOUN'),\n",
       " ('do', 'NOUN'),\n",
       " ('not', 'NOUN'),\n",
       " ('like', 'NOUN'),\n",
       " ('green', 'NOUN'),\n",
       " ('eggs', 'NOUN'),\n",
       " ('and', 'NOUN'),\n",
       " ('ham', 'NOUN'),\n",
       " (',', 'NOUN'),\n",
       " ('I', 'NOUN'),\n",
       " ('do', 'NOUN'),\n",
       " ('not', 'NOUN'),\n",
       " ('like', 'NOUN'),\n",
       " ('them', 'NOUN'),\n",
       " ('Sam', 'NOUN'),\n",
       " ('I', 'NOUN'),\n",
       " ('am', 'NOUN'),\n",
       " ('!', 'NOUN')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_tagger = DefaultTagger('NOUN')\n",
    "default_tagger.tag(tokens)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regexp Tagger permet de tagger grâce au Matching avec une expression régulière"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import RegexpTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = 'Surprisingly, happiness is not always understandable right ?'\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_tagger = RegexpTagger([\n",
    "(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n",
    "(r'(The|the|A|a|An|an)$', 'AT'),   # articles\n",
    "(r'.*able$', 'JJ'),                # adjectives\n",
    "(r'.*ness$', 'NN'),                # nouns formed from adjectives\n",
    "(r'.*ly$', 'RB'),                  # adverbs\n",
    "(r'.*s$', 'NNS'),                  # plural nouns\n",
    "(r'.*ing$', 'VBG'),                # gerunds\n",
    "(r'.*ed$', 'VBD'),                 # past tense verbs\n",
    "(r'\\,|\\?', 'PKT'),                 # punctuation\n",
    "(r'.*', 'NN')                      # nouns (default)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Surprisingly', 'RB'),\n",
       " (',', 'PKT'),\n",
       " ('happiness', 'NN'),\n",
       " ('is', 'NNS'),\n",
       " ('not', 'NN'),\n",
       " ('always', 'NNS'),\n",
       " ('understandable', 'JJ'),\n",
       " ('right', 'NN'),\n",
       " ('?', 'PKT')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons maintenant un exemple de LookUp Tagger qui tag mot par mot, à savoir Unigram Tagger, en cherchant le tag le plus probable pour ce mot dans le corpus <b> brown </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ConditionalFreqDist, UnigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FreqDist(brown.words(categories='news'))\n",
    "cfd = ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
    "most_freq_words = list(fd.keys())[:10000]\n",
    "likely_tags = dict((word, cfd[word].max()) for word in most_freq_words)\n",
    "baseline_tagger = UnigramTagger(model=likely_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on évalue le tagging en calculant le rapport du mobre de tags bien prédits sur le nombre total de tags sur un ensemble de test à savoir les phrases de <b> brown </b> correspondant à la catégorie <b> romance </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7802547770700637"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger.evaluate(brown.tagged_sents(categories='romance'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PPSS'),\n",
       " ('do', 'DO'),\n",
       " ('not', '*'),\n",
       " ('like', 'CS'),\n",
       " ('green', 'NN'),\n",
       " ('eggs', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('ham', None),\n",
       " (',', ','),\n",
       " ('I', 'PPSS'),\n",
       " ('do', 'DO'),\n",
       " ('not', '*'),\n",
       " ('like', 'CS'),\n",
       " ('them', 'PPO'),\n",
       " ('Sam', 'NP'),\n",
       " ('I', 'PPSS'),\n",
       " ('am', 'BEM'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger.tag(word_tokenize('I do not like green eggs and ham, I do not like them Sam I am!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correction orthographique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut utiliser TextBlob qui fournit une interface plus simplifiée de nltk tout en se basant sur les mêmes corpora, méthodes, classes... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl (636kB)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/bestprojectever/anaconda3/lib/python3.7/site-packages (from textblob) (3.4)\n",
      "Requirement already satisfied: six in /home/bestprojectever/anaconda3/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in /home/bestprojectever/anaconda3/lib/python3.7/site-packages (from nltk>=3.1->textblob) (3.4.0.3)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, seul le dictionnaire anglais est présent; pour les autres langues, par exmeple le Français, penser à télécharger d'autres versions de la lib (e.g. <b> textblob-fr </b> )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"thiis is teext written with some bad spell\"\n",
    "textblob_text = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is text written with some bad spell\n"
     ]
    }
   ],
   "source": [
    "print(textblob_text.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regexp Stemmer permet de stemmifier grâce au Matching avec une expression régulière (enlever les parties de mots qui correspondent à la Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = RegexpStemmer('ing$|s$|e$|er$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'port'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.stem('porter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancaster Stemmer permet de stemmifier en se basant sur un algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.stem('saying')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowball Stemmer permet de stemmifier en enlevant ses suffixes aux mots. Ces suffixes sont définis en dur dans la définition de la classe <b> nltk.stem.snowball.SnowballStemmer </b> <br>\n",
    "On peut hériter des Stemmer dans différents langages depuis SnowballStemmer (e.g. FrenchStemmer pour le Français)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = FrenchStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machin'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr.stem('machines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lang detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Langdetect </b> est une lib Python qui permet la reconnaissance de langue d'un paragraphe donné en recherchant chaque mot dans différents dictionnaires de langues. La langue du paragraphe est celle prédominante (le plus de mots dans cette langue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /home/bestprojectever/anaconda3/lib/python3.7/site-packages (1.0.7)\n",
      "Requirement already satisfied: six in /home/bestprojectever/anaconda3/lib/python3.7/site-packages (from langdetect) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a text in english les gars\"\n",
    "langdetect.detect(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tl'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"buongiorno\"\n",
    "langdetect.detect(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple simple de pipeline complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"Big Data is a domaain that haf attracted many companies and it is introducing techniques such as hadoop and spark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On tokenise..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_tokens = word_tokenize(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Big',\n",
       " 'Data',\n",
       " 'is',\n",
       " 'a',\n",
       " 'domaain',\n",
       " 'that',\n",
       " 'haf',\n",
       " 'attracted',\n",
       " 'many',\n",
       " 'companies',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'introducing',\n",
       " 'techniques',\n",
       " 'such',\n",
       " 'as',\n",
       " 'hadoop',\n",
       " 'and',\n",
       " 'spark']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On détecte la langue de la phrase..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langdetect.detect(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enlève les Stopwords..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_phrase_tokens =  [token for token in phrase_tokens if token not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Big',\n",
       " 'Data',\n",
       " 'domaain',\n",
       " 'haf',\n",
       " 'attracted',\n",
       " 'many',\n",
       " 'companies',\n",
       " 'introducing',\n",
       " 'techniques',\n",
       " 'hadoop',\n",
       " 'spark']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_phrase_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On corrige orthographiquement chaque token..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fig',\n",
       " 'Data',\n",
       " 'domain',\n",
       " 'had',\n",
       " 'attracted',\n",
       " 'many',\n",
       " 'companies',\n",
       " 'introducing',\n",
       " 'technique',\n",
       " 'hadoop',\n",
       " 'spark']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_phrase_tokens =  [list(TextBlob(token).correct().words) for token in clean_phrase_tokens]\n",
    "corrected_phrase_tokens = [item for sublist in corrected_phrase_tokens for item in sublist]\n",
    "corrected_phrase_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On stemmifie ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_phrase_tokens = [stemmer.stem(item) for item in corrected_phrase_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On <em> \"dé-tokenise\" </em> les tokens pour reconstruire notre phrase transformée..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fig data domain had attract mani compani introduc techniqu hadoop spark'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(stemmed_phrase_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquons maintenant le Pipeline à notre docx du début..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docxtext_tokens = word_tokenize(docxtext.lower())\n",
    "langdetect.detect(docxtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_docxtext_tokens = [\n",
    "    token for token in docxtext_tokens\n",
    "    if token not in stopwords.words(\"english\")\n",
    "    and token not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_docxtext_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_tag(clean_docxtext_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_docx_tokens =  [list(TextBlob(token).correct().words) for token in clean_docxtext_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_docx_tokens = [item for sublist in corrected_docx_tokens for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant la stemmification, voilà la les mots les plus fréquents dans le docx, on remarque que des mots tels que <b> statistical </b> et <b> statistics </b> sont pris séparément alors qu'ils rendent compte d'un même thème à savoir les statistiques..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'data': 290, 'big': 158, 'statistical': 144, 'organizations': 96, 'sources': 48, 'use': 42, 'statistics': 40, 'may': 40, 'information': 38, 'new': 32, ...})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(corrected_docx_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On exécute maintenant la stemmification..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_docx_tokens = [stemmer.stem(item) for item in corrected_docx_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big',\n",
       " 'big',\n",
       " 'data',\n",
       " 'explor',\n",
       " 'role',\n",
       " 'big',\n",
       " 'data',\n",
       " 'offici',\n",
       " 'statist',\n",
       " 'version',\n",
       " '0.1',\n",
       " 'march',\n",
       " '2014',\n",
       " 'draft',\n",
       " 'review',\n",
       " 'pleas',\n",
       " 'note',\n",
       " 'develop',\n",
       " 'paper',\n",
       " 'work',\n",
       " 'progress',\n",
       " 'intend',\n",
       " 'offici',\n",
       " 'public',\n",
       " 'reflect',\n",
       " 'thought',\n",
       " 'idea',\n",
       " 'gather',\n",
       " 'first',\n",
       " 'virtual',\n",
       " 'spring',\n",
       " 'topic',\n",
       " 'held',\n",
       " 'march',\n",
       " '2014',\n",
       " 'use',\n",
       " 'discuss',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'adapt',\n",
       " 'potenti',\n",
       " 'new',\n",
       " 'futur',\n",
       " 'realiz',\n",
       " 'opportun',\n",
       " 'minim',\n",
       " 'risk',\n",
       " 'purpos',\n",
       " 'paper',\n",
       " 'encourag',\n",
       " 'other',\n",
       " 'join',\n",
       " 'debat',\n",
       " 'identifi',\n",
       " '‘',\n",
       " 'big',\n",
       " '’',\n",
       " 'thing',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'need',\n",
       " 'tackl',\n",
       " 'instruct',\n",
       " 'review',\n",
       " 'templ',\n",
       " 'provid',\n",
       " 'feedback',\n",
       " 'avail',\n",
       " 'http',\n",
       " 'www.once.org/state/platform/display/bigdata/how+big+is+big+data',\n",
       " 'introduct',\n",
       " 'big',\n",
       " 'data',\n",
       " 'increas',\n",
       " 'challeng',\n",
       " 'offici',\n",
       " 'statist',\n",
       " 'communiti',\n",
       " 'need',\n",
       " 'better',\n",
       " 'understand',\n",
       " 'issu',\n",
       " 'develop',\n",
       " 'new',\n",
       " 'method',\n",
       " 'tool',\n",
       " 'idea',\n",
       " 'make',\n",
       " 'effect',\n",
       " 'use',\n",
       " 'big',\n",
       " 'data',\n",
       " 'sourc',\n",
       " '1',\n",
       " 'high-level',\n",
       " 'group',\n",
       " 'modernis',\n",
       " 'statist',\n",
       " 'product',\n",
       " 'servic',\n",
       " '’',\n",
       " 'hug',\n",
       " 'strategi',\n",
       " 'document',\n",
       " 'state',\n",
       " '“',\n",
       " 'product',\n",
       " 'servic',\n",
       " 'must',\n",
       " 'becom',\n",
       " 'easier',\n",
       " 'produc',\n",
       " 'less',\n",
       " 'resource-intens',\n",
       " 'less',\n",
       " 'burdensom',\n",
       " 'data',\n",
       " 'supplier',\n",
       " '”',\n",
       " '“',\n",
       " 'new',\n",
       " 'exist',\n",
       " 'product',\n",
       " 'servic',\n",
       " 'make',\n",
       " 'use',\n",
       " 'vast',\n",
       " 'amount',\n",
       " 'data',\n",
       " 'becom',\n",
       " 'avail',\n",
       " 'provid',\n",
       " 'better',\n",
       " 'measur',\n",
       " 'new',\n",
       " 'aspect',\n",
       " 'societi',\n",
       " '”',\n",
       " 'use',\n",
       " 'big',\n",
       " 'data',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'one',\n",
       " 'way',\n",
       " 'achiev',\n",
       " 'aim',\n",
       " '2',\n",
       " 'alreadi',\n",
       " 'number',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'investig',\n",
       " 'report',\n",
       " 'use',\n",
       " 'big',\n",
       " 'data',\n",
       " 'tend',\n",
       " 'opportunist',\n",
       " 'use',\n",
       " 'big',\n",
       " 'data',\n",
       " '–',\n",
       " 'driven',\n",
       " 'increas',\n",
       " 'avail',\n",
       " 'data',\n",
       " 'sourc',\n",
       " 'need',\n",
       " 'provid',\n",
       " 'statist',\n",
       " 'short',\n",
       " 'notic',\n",
       " 'exampl',\n",
       " 'case',\n",
       " 'natur',\n",
       " 'disast',\n",
       " 'offici',\n",
       " 'statist',\n",
       " 'communiti',\n",
       " 'need',\n",
       " 'take',\n",
       " 'strateg',\n",
       " 'look',\n",
       " 'big',\n",
       " 'data',\n",
       " 'mean',\n",
       " 'communiti',\n",
       " '3',\n",
       " '2014',\n",
       " 'hug',\n",
       " 'sponsor',\n",
       " 'project',\n",
       " 'sign',\n",
       " 'aspir',\n",
       " 'focus',\n",
       " 'role',\n",
       " 'big',\n",
       " 'data',\n",
       " 'offici',\n",
       " 'statist',\n",
       " 'project',\n",
       " 'aim',\n",
       " 'analys',\n",
       " 'major',\n",
       " 'strateg',\n",
       " 'question',\n",
       " 'pose',\n",
       " 'emerg',\n",
       " 'big',\n",
       " 'data',\n",
       " 'project',\n",
       " 'also',\n",
       " 'practic',\n",
       " 'element',\n",
       " 'web-access',\n",
       " 'environ',\n",
       " 'storag',\n",
       " 'analysi',\n",
       " 'large-scal',\n",
       " 'dataset',\n",
       " 'creat',\n",
       " 'use',\n",
       " '‘',\n",
       " 'sandbag',\n",
       " '’',\n",
       " 'collabor',\n",
       " 'across',\n",
       " 'particip',\n",
       " 'institut',\n",
       " '4',\n",
       " 'order',\n",
       " 'progress',\n",
       " 'work',\n",
       " 'project',\n",
       " 'virtual',\n",
       " 'spring',\n",
       " 'held',\n",
       " 'week',\n",
       " '10',\n",
       " '–',\n",
       " '14',\n",
       " 'march',\n",
       " '2014',\n",
       " 'paper',\n",
       " 'summaris',\n",
       " 'discuss',\n",
       " 'held',\n",
       " 'big',\n",
       " 'data',\n",
       " 'virtual',\n",
       " 'spring',\n",
       " 'purpos',\n",
       " 'find',\n",
       " 'answer',\n",
       " 'enumer',\n",
       " 'equal',\n",
       " 'opportun',\n",
       " 'challeng',\n",
       " 'issu',\n",
       " 'question',\n",
       " 'use',\n",
       " 'big',\n",
       " 'data',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'rais',\n",
       " 'paper',\n",
       " 'compris',\n",
       " 'four',\n",
       " 'main',\n",
       " 'part',\n",
       " 'definit',\n",
       " 'sourc',\n",
       " 'big',\n",
       " 'data',\n",
       " 'context',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'import',\n",
       " 'use',\n",
       " 'big',\n",
       " 'data',\n",
       " 'term',\n",
       " 'demand',\n",
       " 'valu',\n",
       " 'chang',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'may',\n",
       " 'need',\n",
       " 'big',\n",
       " 'data',\n",
       " 'partnership',\n",
       " 'may',\n",
       " 'necessari',\n",
       " 'achiev',\n",
       " 'goal',\n",
       " 'use',\n",
       " 'big',\n",
       " 'data',\n",
       " '5',\n",
       " 'output',\n",
       " 'virtual',\n",
       " 'spring',\n",
       " 'paper',\n",
       " 'follow',\n",
       " 'comment',\n",
       " 'statist',\n",
       " 'communiti',\n",
       " 'issu',\n",
       " 'examin',\n",
       " 'paper',\n",
       " 'follow',\n",
       " 'workshop',\n",
       " 'rome',\n",
       " '2-3',\n",
       " 'april',\n",
       " 'number',\n",
       " 'virtual',\n",
       " 'task',\n",
       " 'tear',\n",
       " 'form',\n",
       " 'tear',\n",
       " 'work',\n",
       " 'prioriti',\n",
       " 'issu',\n",
       " 'identifi',\n",
       " 'spring',\n",
       " 'workshop',\n",
       " 'requir',\n",
       " 'produc',\n",
       " 'output',\n",
       " 'typic',\n",
       " 'form',\n",
       " 'guidelin',\n",
       " 'recommend',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'novemb',\n",
       " '2014',\n",
       " 'time',\n",
       " 'result',\n",
       " 'big',\n",
       " 'data',\n",
       " 'sandbag',\n",
       " 'also',\n",
       " 'releas',\n",
       " 'big',\n",
       " 'data',\n",
       " 'context',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'definit',\n",
       " '6',\n",
       " 'numer',\n",
       " 'exist',\n",
       " 'definit',\n",
       " 'big',\n",
       " 'data',\n",
       " 'avail',\n",
       " 'definit',\n",
       " 'usual',\n",
       " 'split',\n",
       " 'two',\n",
       " 'part',\n",
       " 'breakdown',\n",
       " 'differ',\n",
       " 'data',\n",
       " 'sourc',\n",
       " 'view',\n",
       " 'big',\n",
       " 'data',\n",
       " 'techniqu',\n",
       " 'methodolog',\n",
       " 'appli',\n",
       " 'data',\n",
       " 'differ',\n",
       " 'tradit',\n",
       " 'treatment',\n",
       " 'data',\n",
       " '7',\n",
       " 'propos',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'regard',\n",
       " 'big',\n",
       " 'data',\n",
       " 'data',\n",
       " 'difficult',\n",
       " 'collect',\n",
       " 'store',\n",
       " 'process',\n",
       " 'within',\n",
       " 'convent',\n",
       " 'system',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'either',\n",
       " 'volum',\n",
       " 'veloc',\n",
       " 'structur',\n",
       " 'varieti',\n",
       " 'requir',\n",
       " 'adopt',\n",
       " 'new',\n",
       " 'statist',\n",
       " 'softwar',\n",
       " 'process',\n",
       " 'techniqu',\n",
       " 'and/or',\n",
       " 'infrastructur',\n",
       " 'enabl',\n",
       " 'cost-effect',\n",
       " 'insight',\n",
       " 'made',\n",
       " '8',\n",
       " 'unlik',\n",
       " 'survey',\n",
       " 'data',\n",
       " 'sourc',\n",
       " 'statist',\n",
       " 'industri',\n",
       " 'could',\n",
       " 'consid',\n",
       " 'big',\n",
       " 'data',\n",
       " 'bar',\n",
       " 'larg',\n",
       " 'census',\n",
       " 'data',\n",
       " 'collect',\n",
       " '9',\n",
       " 'mani',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'current',\n",
       " 'explor',\n",
       " 'and/or',\n",
       " 'use',\n",
       " 'administr',\n",
       " 'data',\n",
       " 'replac',\n",
       " 'survey',\n",
       " 'inform',\n",
       " 'administr',\n",
       " 'data',\n",
       " 'potenti',\n",
       " 'enter',\n",
       " 'big',\n",
       " 'data',\n",
       " 'problem',\n",
       " 'domain',\n",
       " 'case',\n",
       " 'data',\n",
       " 'might',\n",
       " 'expand',\n",
       " 'sever',\n",
       " 'databas',\n",
       " 'link',\n",
       " 'vast',\n",
       " 'amount',\n",
       " 'data',\n",
       " 'lead',\n",
       " 'solut',\n",
       " 'requir',\n",
       " 'big',\n",
       " 'data',\n",
       " 'type',\n",
       " 'process',\n",
       " 'power',\n",
       " 'solut',\n",
       " 'big',\n",
       " 'data',\n",
       " 'sourc',\n",
       " '10',\n",
       " 'big',\n",
       " 'data',\n",
       " 'aris',\n",
       " 'mani',\n",
       " 'sourc',\n",
       " 'follow',\n",
       " 'paragraph',\n",
       " 'set',\n",
       " 'three',\n",
       " 'type',\n",
       " 'data',\n",
       " 'sourc',\n",
       " 'view',\n",
       " 'big',\n",
       " 'data',\n",
       " 'annex',\n",
       " 'detail',\n",
       " 'apolog',\n",
       " 'found',\n",
       " 'provid',\n",
       " 'inform',\n",
       " 'group',\n",
       " 'human-sourc',\n",
       " 'inform',\n",
       " 'social',\n",
       " 'network',\n",
       " 'process-medit',\n",
       " 'data',\n",
       " 'tradit',\n",
       " 'busi',\n",
       " 'system',\n",
       " 'webster',\n",
       " 'machine-gener',\n",
       " 'data',\n",
       " 'automat',\n",
       " 'system',\n",
       " '11',\n",
       " 'import',\n",
       " 'distinguish',\n",
       " 'differ',\n",
       " 'type',\n",
       " 'data',\n",
       " 'sourc',\n",
       " 'bring',\n",
       " 'differ',\n",
       " 'set',\n",
       " 'consider',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'tabl',\n",
       " '1',\n",
       " 'set',\n",
       " 'differ',\n",
       " 'consider',\n",
       " 'type',\n",
       " 'data',\n",
       " 'sourc',\n",
       " 'tabl',\n",
       " '1',\n",
       " 'differ',\n",
       " 'characterist',\n",
       " 'data',\n",
       " 'sourc',\n",
       " 'type',\n",
       " '‘',\n",
       " 'big',\n",
       " '’',\n",
       " 'big',\n",
       " 'data',\n",
       " 'statist',\n",
       " 'organ',\n",
       " '12',\n",
       " 'section',\n",
       " 'import',\n",
       " 'big',\n",
       " 'data',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'examin',\n",
       " 'demand',\n",
       " 'big',\n",
       " 'data',\n",
       " 'valu',\n",
       " 'proposit',\n",
       " 'question',\n",
       " 'whether',\n",
       " 'short',\n",
       " 'term',\n",
       " 'trend',\n",
       " 'someth',\n",
       " 'becom',\n",
       " 'integr',\n",
       " 'work',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'discuss',\n",
       " 'demand',\n",
       " 'big',\n",
       " 'data',\n",
       " '13',\n",
       " 'demand',\n",
       " 'drive',\n",
       " 'need',\n",
       " 'big',\n",
       " 'data',\n",
       " 'broad',\n",
       " 'split',\n",
       " 'three',\n",
       " 'categori',\n",
       " 'reput',\n",
       " 'driver',\n",
       " 'effici',\n",
       " 'driver',\n",
       " 'inform',\n",
       " 'need',\n",
       " 'driver',\n",
       " '14',\n",
       " 'reput',\n",
       " 'driver',\n",
       " 'big',\n",
       " 'data',\n",
       " 'centr',\n",
       " 'around',\n",
       " 'notion',\n",
       " 'big',\n",
       " 'data',\n",
       " 'import',\n",
       " 'develop',\n",
       " 'potenti',\n",
       " 'signific',\n",
       " 'impact',\n",
       " 'statist',\n",
       " 'industri',\n",
       " 'import',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'continu',\n",
       " 'demonstr',\n",
       " 'relev',\n",
       " 'remain',\n",
       " 'competit',\n",
       " 'emerg',\n",
       " 'sourc',\n",
       " 'data',\n",
       " 'govern',\n",
       " 'continu',\n",
       " 'see',\n",
       " 'valu',\n",
       " 'offici',\n",
       " 'statist',\n",
       " 'driver',\n",
       " 'seek',\n",
       " 'exploit',\n",
       " 'new',\n",
       " 'opportun',\n",
       " 'keep',\n",
       " 'pace',\n",
       " 'possibl',\n",
       " 'lead',\n",
       " 'data-tor',\n",
       " 'approach',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'ask',\n",
       " 'make',\n",
       " 'use',\n",
       " 'new',\n",
       " 'sourc',\n",
       " 'energi',\n",
       " 'consumpt',\n",
       " 'statist',\n",
       " 'tend',\n",
       " 'credit',\n",
       " 'card',\n",
       " 'consum',\n",
       " 'loyalti',\n",
       " 'inform',\n",
       " 'web',\n",
       " 'search',\n",
       " 'inform',\n",
       " 'satellit',\n",
       " 'ground',\n",
       " 'senior',\n",
       " 'data',\n",
       " 'mobil',\n",
       " 'devic',\n",
       " 'locat',\n",
       " 'data',\n",
       " '15',\n",
       " 'effici',\n",
       " 'driver',\n",
       " 'aris',\n",
       " 'challeng',\n",
       " 'face',\n",
       " 'mani',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'reduc',\n",
       " 'cost',\n",
       " 'time',\n",
       " 'produc',\n",
       " 'improv',\n",
       " 'output',\n",
       " 'emerg',\n",
       " 'data',\n",
       " 'sourc',\n",
       " 'technolog',\n",
       " 'methodolog',\n",
       " 'world',\n",
       " 'big',\n",
       " 'data',\n",
       " 'sought',\n",
       " 'solv',\n",
       " 'exist',\n",
       " 'issu',\n",
       " 'reduc',\n",
       " 'aspect',\n",
       " 'statist',\n",
       " 'busi',\n",
       " 'process',\n",
       " 'associ',\n",
       " 'higher',\n",
       " 'cost',\n",
       " 'exampl',\n",
       " 'big',\n",
       " 'data',\n",
       " 'may',\n",
       " 'abl',\n",
       " 'add',\n",
       " 'valu',\n",
       " 'sampl',\n",
       " 'frame',\n",
       " 'regist',\n",
       " 'creation',\n",
       " '–',\n",
       " 'identifi',\n",
       " 'provid',\n",
       " 'inform',\n",
       " 'survey',\n",
       " 'popul',\n",
       " 'unit',\n",
       " 'full',\n",
       " 'partial',\n",
       " 'data',\n",
       " 'substitut',\n",
       " '–',\n",
       " 'replac',\n",
       " 'survey',\n",
       " 'collect',\n",
       " 'reduc',\n",
       " 'sampl',\n",
       " 'size',\n",
       " 'simplifi',\n",
       " 'survey',\n",
       " 'instrument',\n",
       " 'data',\n",
       " 'confront',\n",
       " 'imput',\n",
       " 'edit',\n",
       " '–',\n",
       " 'ensur',\n",
       " 'valid',\n",
       " 'consist',\n",
       " 'accuraci',\n",
       " 'survey',\n",
       " 'data',\n",
       " '16',\n",
       " 'need',\n",
       " 'new',\n",
       " 'statist',\n",
       " 'statist',\n",
       " 'improv',\n",
       " 'liveli',\n",
       " 'relev',\n",
       " 'also',\n",
       " 'key',\n",
       " 'driver',\n",
       " 'may',\n",
       " 'includ',\n",
       " 'make',\n",
       " 'use',\n",
       " 'new',\n",
       " 'data',\n",
       " 'sourc',\n",
       " 'fill',\n",
       " 'particular',\n",
       " 'inform',\n",
       " 'need',\n",
       " 'extend',\n",
       " 'exist',\n",
       " 'measur',\n",
       " 'econom',\n",
       " 'social',\n",
       " 'environment',\n",
       " 'phenomena',\n",
       " 'high',\n",
       " 'qualiti',\n",
       " 'use',\n",
       " 'polici',\n",
       " 'make',\n",
       " 'altern',\n",
       " 'may',\n",
       " 'enabl',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'produc',\n",
       " 'statist',\n",
       " 'high',\n",
       " 'qualiti',\n",
       " 'less',\n",
       " 'appropri',\n",
       " 'meet',\n",
       " 'public',\n",
       " 'demand',\n",
       " 'issu',\n",
       " 'day',\n",
       " 'may',\n",
       " 'rang',\n",
       " 'demand',\n",
       " 'assist',\n",
       " 'use',\n",
       " 'big',\n",
       " 'data',\n",
       " 'improv',\n",
       " 'liveli',\n",
       " 'output',\n",
       " 'enhanc',\n",
       " 'relev',\n",
       " 'granular',\n",
       " 'output',\n",
       " 'increas',\n",
       " 'accuraci',\n",
       " 'consist',\n",
       " 'output',\n",
       " 'valu',\n",
       " 'proposit',\n",
       " '17',\n",
       " 'big',\n",
       " 'data',\n",
       " 'present',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'tripl',\n",
       " 'opportun',\n",
       " 'data',\n",
       " 'sourc',\n",
       " 'technolog',\n",
       " 'methodolog',\n",
       " 'describ',\n",
       " 'definit',\n",
       " 'section',\n",
       " 'big',\n",
       " 'data',\n",
       " 'singl',\n",
       " 'thing',\n",
       " 'collect',\n",
       " 'data',\n",
       " 'sourc',\n",
       " 'technolog',\n",
       " 'methodolog',\n",
       " 'emerg',\n",
       " 'exploit',\n",
       " 'exponenti',\n",
       " 'growth',\n",
       " 'data',\n",
       " 'creation',\n",
       " 'past',\n",
       " 'decad',\n",
       " '18',\n",
       " 'present',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'opportun',\n",
       " 'utilis',\n",
       " 'aspect',\n",
       " 'support',\n",
       " 'new',\n",
       " 'exist',\n",
       " 'busi',\n",
       " 'need',\n",
       " 'may',\n",
       " 'take',\n",
       " 'form',\n",
       " 'new',\n",
       " 'product',\n",
       " 'base',\n",
       " 'larg',\n",
       " 'volum',\n",
       " 'data',\n",
       " 'non-tradit',\n",
       " 'sourc',\n",
       " 'also',\n",
       " 'implement',\n",
       " 'new',\n",
       " 'methodolog',\n",
       " 'improv',\n",
       " 'effici',\n",
       " 'qualiti',\n",
       " 'exist',\n",
       " 'busi',\n",
       " 'domain',\n",
       " '19',\n",
       " 'big',\n",
       " 'data',\n",
       " 'potenti',\n",
       " 'creat',\n",
       " 'water',\n",
       " 'moment',\n",
       " 'critic',\n",
       " 'turn',\n",
       " 'point',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'encompass',\n",
       " 'three',\n",
       " 'area',\n",
       " 'unlock',\n",
       " 'potenti',\n",
       " 'provid',\n",
       " 'analyt',\n",
       " 'insight',\n",
       " 'previous',\n",
       " 'feasibl',\n",
       " 'limit',\n",
       " 'associ',\n",
       " 'cost',\n",
       " 'three',\n",
       " 'area',\n",
       " 'inextric',\n",
       " 'connect',\n",
       " 'core',\n",
       " 'busi',\n",
       " 'statist',\n",
       " 'organ',\n",
       " '20',\n",
       " 'two',\n",
       " 'key',\n",
       " 'perspect',\n",
       " 'assess',\n",
       " 'valu',\n",
       " 'big',\n",
       " 'data',\n",
       " 'valu',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'relat',\n",
       " 'oper',\n",
       " 'effici',\n",
       " 'valu',\n",
       " 'consum',\n",
       " 'inform',\n",
       " 'product',\n",
       " 'servic',\n",
       " 'perspect',\n",
       " 'addit',\n",
       " 'inform',\n",
       " 'privaci',\n",
       " 'secur',\n",
       " 'legisl',\n",
       " 'consider',\n",
       " 'valu',\n",
       " 'statist',\n",
       " 'product',\n",
       " '21',\n",
       " 'busi',\n",
       " 'valu',\n",
       " 'assum',\n",
       " 'assur',\n",
       " 'assist',\n",
       " 'capabl',\n",
       " 'invest',\n",
       " 'decis',\n",
       " 'statist',\n",
       " 'organ',\n",
       " 'need',\n",
       " 'abl',\n",
       " 'demonstr',\n",
       " 'use',\n",
       " 'new',\n",
       " 'big',\n",
       " 'data',\n",
       " 'sourc',\n",
       " '–',\n",
       " 'either',\n",
       " 'alon',\n",
       " 'energi',\n",
       " 'exist',\n",
       " 'data',\n",
       " 'sourc',\n",
       " '–',\n",
       " 'actual',\n",
       " 'improv',\n",
       " 'end-to-end',\n",
       " 'statist',\n",
       " 'outcom',\n",
       " 'valu',\n",
       " 'proposit',\n",
       " 'express',\n",
       " 'term',\n",
       " 'object',\n",
       " 'criterion',\n",
       " 'cost',\n",
       " 'sustain',\n",
       " 'statist',\n",
       " 'output',\n",
       " 'accuraci',\n",
       " 'relev',\n",
       " 'consist',\n",
       " 'interpret',\n",
       " 'liveli',\n",
       " 'output',\n",
       " 'stipul',\n",
       " 'establish',\n",
       " 'qualiti',\n",
       " 'framework',\n",
       " 'like',\n",
       " 'avail',\n",
       " 'sourc',\n",
       " 'involv',\n",
       " 'differ',\n",
       " 'set',\n",
       " 'compromis',\n",
       " 'e.g',\n",
       " 'time',\n",
       " ...]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_docx_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après stemmification, les statistiques (racine <b> statist </b>) sont montées plus haut grâce au regroupement ayant été fait sur les mots dérivant de cette racine... ce qui permet de mettre en avant les statistiques comme thème / sujet très caractéristique du document..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'data': 291, 'statist': 184, 'big': 158, 'organ': 111, 'use': 74, 'sourc': 66, 'need': 47, 'may': 40, 'inform': 40, 'new': 32, ...})"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(stemmed_docx_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
