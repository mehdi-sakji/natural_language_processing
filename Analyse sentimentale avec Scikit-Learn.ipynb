{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse sentimentale avec Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depuis NLTK, on télécharge et importe un corpus contenant des commentaires sur des films et qui sont annotés; c'est à dire pour chaque commentaire on a l'info (sous forme de flag  ou 1) si c'est positif ou négatif..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue une petite transformation (Reshaping) de données pour les préparer en entrée de notre algorithme de classification..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_train = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un petit <b> shuffle </b>  est nécessaire pour que, plus tard,  on ait un entraînement et un test qui ne dépendent pas de la séquence de ces commentaires..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(movie_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre dataset contient 2000 commentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_train_data = [\" \".join(item[0]) for item in movie_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_train_targets = [item[1] for item in movie_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numérisation des commentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va numériser les commentaires en utilisant le procédé TF-IDF. On commence par calculer la fréquence <b> TF </b> de chaque mot dans chaque commentaire, puis on pondère ces fréquences par le terme <b> IDF </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_vec = CountVectorizer(min_df=2, tokenizer=word_tokenize)\n",
    "movie_counts = movie_vec.fit_transform(movie_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "movie_tfidf = tfidf_transformer.fit_transform(movie_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décommenter pour afficher les poids TF-IDF résultants\n",
    "# movie_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 24160)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En sortie de TF-IDF, on a une dimensionnalité de 24160, qui est très élevée. On réduit celle là grâce à l'algo PCA (avec dimensionnalité cible égale à 300) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "pca_model = TruncatedSVD(n_components = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_features = pd.DataFrame(pca_model.fit_transform(movie_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.616673</td>\n",
       "      <td>0.187617</td>\n",
       "      <td>0.020689</td>\n",
       "      <td>-0.010779</td>\n",
       "      <td>-0.043433</td>\n",
       "      <td>-0.046580</td>\n",
       "      <td>0.029482</td>\n",
       "      <td>-0.080083</td>\n",
       "      <td>0.014084</td>\n",
       "      <td>-0.023268</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016434</td>\n",
       "      <td>-0.002631</td>\n",
       "      <td>0.007213</td>\n",
       "      <td>-0.029626</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.007734</td>\n",
       "      <td>0.019336</td>\n",
       "      <td>-0.003916</td>\n",
       "      <td>0.016984</td>\n",
       "      <td>0.000502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550033</td>\n",
       "      <td>-0.010215</td>\n",
       "      <td>-0.016043</td>\n",
       "      <td>-0.047319</td>\n",
       "      <td>-0.032550</td>\n",
       "      <td>0.005165</td>\n",
       "      <td>0.014986</td>\n",
       "      <td>-0.021206</td>\n",
       "      <td>0.022857</td>\n",
       "      <td>-0.016931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000965</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>0.014166</td>\n",
       "      <td>0.017873</td>\n",
       "      <td>-0.010285</td>\n",
       "      <td>-0.013008</td>\n",
       "      <td>0.027376</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>0.042732</td>\n",
       "      <td>-0.034176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.563653</td>\n",
       "      <td>-0.095314</td>\n",
       "      <td>-0.052334</td>\n",
       "      <td>0.137567</td>\n",
       "      <td>-0.036462</td>\n",
       "      <td>0.078363</td>\n",
       "      <td>0.063313</td>\n",
       "      <td>0.023514</td>\n",
       "      <td>0.032131</td>\n",
       "      <td>-0.034066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>0.055028</td>\n",
       "      <td>0.045293</td>\n",
       "      <td>-0.004875</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>0.010198</td>\n",
       "      <td>-0.012674</td>\n",
       "      <td>0.049590</td>\n",
       "      <td>-0.031206</td>\n",
       "      <td>0.001216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.579325</td>\n",
       "      <td>-0.064473</td>\n",
       "      <td>-0.101694</td>\n",
       "      <td>-0.060615</td>\n",
       "      <td>0.073223</td>\n",
       "      <td>-0.115141</td>\n",
       "      <td>0.039840</td>\n",
       "      <td>-0.022482</td>\n",
       "      <td>0.044138</td>\n",
       "      <td>-0.012617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023026</td>\n",
       "      <td>-0.001419</td>\n",
       "      <td>0.017665</td>\n",
       "      <td>0.010269</td>\n",
       "      <td>0.014328</td>\n",
       "      <td>-0.003456</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>0.007905</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>0.001501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.570528</td>\n",
       "      <td>-0.113899</td>\n",
       "      <td>0.007387</td>\n",
       "      <td>0.017383</td>\n",
       "      <td>0.027013</td>\n",
       "      <td>-0.058169</td>\n",
       "      <td>-0.055419</td>\n",
       "      <td>-0.010109</td>\n",
       "      <td>-0.058541</td>\n",
       "      <td>-0.026143</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032448</td>\n",
       "      <td>0.008412</td>\n",
       "      <td>0.010092</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>-0.020864</td>\n",
       "      <td>-0.018297</td>\n",
       "      <td>-0.024874</td>\n",
       "      <td>-0.007952</td>\n",
       "      <td>0.005767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.616673  0.187617  0.020689 -0.010779 -0.043433 -0.046580  0.029482   \n",
       "1  0.550033 -0.010215 -0.016043 -0.047319 -0.032550  0.005165  0.014986   \n",
       "2  0.563653 -0.095314 -0.052334  0.137567 -0.036462  0.078363  0.063313   \n",
       "3  0.579325 -0.064473 -0.101694 -0.060615  0.073223 -0.115141  0.039840   \n",
       "4  0.570528 -0.113899  0.007387  0.017383  0.027013 -0.058169 -0.055419   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0 -0.080083  0.014084 -0.023268  ... -0.016434 -0.002631  0.007213 -0.029626   \n",
       "1 -0.021206  0.022857 -0.016931  ... -0.000965  0.008323  0.014166  0.017873   \n",
       "2  0.023514  0.032131 -0.034066  ...  0.004594  0.055028  0.045293 -0.004875   \n",
       "3 -0.022482  0.044138 -0.012617  ... -0.023026 -0.001419  0.017665  0.010269   \n",
       "4 -0.010109 -0.058541 -0.026143  ... -0.032448  0.008412  0.010092  0.003922   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.002070  0.007734  0.019336 -0.003916  0.016984  0.000502  \n",
       "1 -0.010285 -0.013008  0.027376  0.009148  0.042732 -0.034176  \n",
       "2  0.003672  0.010198 -0.012674  0.049590 -0.031206  0.001216  \n",
       "3  0.014328 -0.003456 -0.000146  0.007905 -0.000146  0.001501  \n",
       "4  0.000785 -0.020864 -0.018297 -0.024874 -0.007952  0.005767  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe le modèle de SVM linéaire depuis Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue le Train Test Split en attribuant 20% des échantillons choisis aléatoirement à l'ensemble de test et le reste (80%) à l'ensemble de Train (apprentissage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    pca_features, movie_train_targets, test_size = 0.20, random_state = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraîne le modèle SVM linéaire sur les données d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC().fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue des prédictions sur tout l'ensemble de test et on calcule un score de précision (accuracy) comme étant le rapport du nombre de bonnes prédictions sur la taille de l'ensemble <br>\n",
    "Ce calcul s'effectue grâce à la fonction <b> sklearn.metrics.accuracy_score </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(docs_test)\n",
    "sklearn.metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi calculer une matrice de confusion à base de ces prédictions sur l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[169,  30],\n",
       "       [ 31, 170]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
